    tokenizer ->  parser ->    compiler ->  linker ->  assembler
text   ->   tokens  -> syntax tree -> object -> program -> machine code

Tokenizer:
----------
Converts text into tokens. E.g. "function f()" -> "function" "f" "(" ")".
Simple state-machine scanner. Nothing much to see here.

Parser:
-------

Converts tokens into abstract syntax tree - checks and implements syntax. *That's it.* Don't be tempted to perform any compilation steps in the parser! 

Compiler:
---------

Reads in the tree, produces an object.
Includes:
- assigning GUIDs to each variable instance
- uses these to produce local links, resolved to machine address links by linker.
- Produces an object for the file.
- object contains a list of vars declared, a list of statements, a list of required symbols, a list of defined symbols.
- object can contain more objects. ?
- Perform any optimizations/contractions?
- Objects are serializable and deserializable (JSON? ;) - can be saved/loaded to/from files.

example:
function f(a)
{
  var a;
  //implementation follows, refers to a
}

We get the tree for this definition from the parser, but the parser will not tell us which a is referred to by the implementation.
As it climbs the tree again, the compiler will give unique IDs (e.g. a@1234, a@5678) to the variables, and keep track of which variable the name refers to at a given time. It can then replace each name with the id it really refers to, and pass the whole lot off to the linker to resolve the unique symbols to machine addresses.

Linker:
-------

- Reads in all the objects, checks that all linked objects have matching definitions.
- Expands macros, links functions in.
- Converts location symbols (e.g. variable names) into actual allocated machine addresses.

